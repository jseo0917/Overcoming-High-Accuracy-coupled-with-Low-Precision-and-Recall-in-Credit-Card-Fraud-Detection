# Overcoming-High-Accuracy-coupled-with-Low-Precision-and-Recall-in-Credit-Card-Fraud-Detection
Overcoming High Accuracy coupled with Low Precision and Recall in Credit Card Fraud Detection
Low precision and recall but high accuracy is often caused by unbalanced data. A classic solution is to either under sample the majority class or oversample the minority class. However, a side effect of under-sampling is that one can end up with much less data overall to train their classifier(s) and can lessen the potential for improving the precision and recall. Meanwhile, oversampling can cause overfitting. In this paper we found under sampling to be more effective for our data. Hence, we constructed an ensemble learner to cope with the side effects of under-sampling in attempt to improve the precision and recall of our classifier while ensuring that the accuracy still stays high. The baseline used to evaluate our ensemble classifier is a model1 that uses Local Outlier Factor and Isolation Forest to classify transactions. The baseline model is adversely affected by data imbalance and has a low precision combined with high accuracy. The dataset we will be using in this paper is from Kaggle which contains 284,807 transactions. Of those transactions, 492 are fraudulent.
